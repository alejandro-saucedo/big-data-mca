{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e36cf2",
   "metadata": {},
   "source": [
    "Before running this example you need to perform the following steps:\n",
    "\n",
    " - Inside the kafka container, create the traffic sensor topic by running:\n",
    " \n",
    "   `kafka-topics.sh --create --replication-factor 1 --bootstrap-server localhost:9092 --topic traffic_sensor`\n",
    "   \n",
    "\n",
    " - Recreate the file work/data/AGOSTO_PARQUET_FINAL.zip by running:\n",
    " \n",
    "   `cat work/data/AGOSTO_PARQUET_FINAL.zip-a* > work/data/AGOSTO_PARQUET_FINAL.zip`\n",
    "   \n",
    "   \n",
    " - Unzip work/data/AGOSTO_PARQUET_FINAL.zip. The following path must exist after unzipping: work/data/AGOSTO_PARQUET_FINAL\n",
    "   \n",
    "\n",
    " - Load the data to kafka by running the spark job at work/src/main/python/insert_traffic_topic.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d88a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # when set, DataFrames are eagerly evaluated, no need to call show\n",
    "data = spark.read.load('/work/data/AGOSTO_2022_PARQUET_FINAL')\n",
    "#data # it will display the DataFrame's contents when spark.sql.repl.eagerEval.enabled is True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f803a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS = \"kafka:9092\"\n",
    "KAFKA_TOPIC = \"traffic_sensor\"\n",
    "SCHEMA = data.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b4f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EQP_ID: long (nullable = true)\n",
      " |-- DATE_TIME: timestamp (nullable = true)\n",
      " |-- MILLISECOND: long (nullable = true)\n",
      " |-- CLASSIFICATION: string (nullable = true)\n",
      " |-- ROAD_LANE: long (nullable = true)\n",
      " |-- ADDRESS_ID: long (nullable = true)\n",
      " |-- ROAD_SPEED: string (nullable = true)\n",
      " |-- VEHICLE_SPEED: string (nullable = true)\n",
      " |-- VEHICLE_LENGTH: string (nullable = true)\n",
      " |-- SERIAL_NUMBER: long (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- ADDRESS: string (nullable = true)\n",
      " |-- DIRECTION: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26206599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic_stream = spark\\\n",
    "    .readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC)\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f7dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_traffic_stream = df_traffic_stream\\\n",
    "    .select(\n",
    "        F.from_json(\n",
    "            # decode string as iso-8859-1\n",
    "            F.decode(F.col(\"value\"), \"iso-8859-1\"),\n",
    "            SCHEMA\n",
    "        ).alias(\"value\")\n",
    "    )\\\n",
    "    .select(\"value.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffce35d",
   "metadata": {},
   "source": [
    "## Count each vehicle type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4d9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_type_stream = df_traffic_stream.select(F.col(\"classification\").alias(\"vehicle_type\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5321df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/28 04:18:02 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c34ca563-3613-4f26-9b10-dd1967f06b9d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff7e8359750>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicle_type_stream.groupBy(\"vehicle_type\")\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"vehicle_type_count\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "147853e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b793b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dedfafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|      vehicle_type|count|\n",
      "+------------------+-----+\n",
      "|         AUTOMÓVEL|80507|\n",
      "|        INDEFINIDO|    2|\n",
      "|              MOTO|13609|\n",
      "|CAMINHÃO / ÔNIBUS | 5882|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    clear_output(wait=True)\n",
    "    spark.sql(\"SELECT * FROM vehicle_type_count\").show()\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e25bf",
   "metadata": {},
   "source": [
    "## Tumbling Windows\n",
    "\n",
    "Count records for each 5-minutes window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22fe4975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/28 04:20:38 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e487794b-1f77-43d3-8a42-4832c5c242e4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff7e8030190>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traffic_stream\\\n",
    "    .groupBy(\n",
    "        F.window(\"DATE_TIME\", \"5 minutes\", )\n",
    "    )\\\n",
    "    .count()\\\n",
    "    .writeStream\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"tumbling_windows\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bb2cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----+\n",
      "|window                                    |count|\n",
      "+------------------------------------------+-----+\n",
      "|[2022-08-04 11:15:00, 2022-08-04 11:20:00]|10669|\n",
      "|[2022-08-04 11:45:00, 2022-08-04 11:50:00]|4767 |\n",
      "|[2022-08-04 11:10:00, 2022-08-04 11:15:00]|10730|\n",
      "|[2022-08-04 11:00:00, 2022-08-04 11:05:00]|9578 |\n",
      "|[2022-08-04 11:25:00, 2022-08-04 11:30:00]|10625|\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:10:00]|10747|\n",
      "|[2022-08-04 11:20:00, 2022-08-04 11:25:00]|10786|\n",
      "|[2022-08-04 11:40:00, 2022-08-04 11:45:00]|10705|\n",
      "|[2022-08-04 11:35:00, 2022-08-04 11:40:00]|10957|\n",
      "|[2022-08-04 11:30:00, 2022-08-04 11:35:00]|10436|\n",
      "+------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    clear_output(wait=True)\n",
    "    spark.sql(\"SELECT * FROM tumbling_windows limit 10\").show(truncate=False)\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e962890",
   "metadata": {},
   "source": [
    "## Slinding Windows\n",
    "Count records for each 10-minutes window each 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35671200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/28 04:21:44 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-02af6434-8857-49d0-969b-63d622b3a7ee. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff7e7fe4c90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traffic_stream\\\n",
    "    .groupBy(\n",
    "        F.window(\"DATE_TIME\", \"10 minutes\", \"5 minutes\")\n",
    "    )\\\n",
    "    .count()\\\n",
    "    .sort('window')\\\n",
    "    .writeStream\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"sliding_window\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f23fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----+\n",
      "|window                                    |count|\n",
      "+------------------------------------------+-----+\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|21477|\n",
      "|[2022-08-04 11:10:00, 2022-08-04 11:20:00]|21399|\n",
      "|[2022-08-04 11:15:00, 2022-08-04 11:25:00]|21455|\n",
      "|[2022-08-04 11:35:00, 2022-08-04 11:45:00]|21662|\n",
      "|[2022-08-04 11:40:00, 2022-08-04 11:50:00]|15472|\n",
      "|[2022-08-04 11:45:00, 2022-08-04 11:55:00]|4767 |\n",
      "|[2022-08-04 10:55:00, 2022-08-04 11:05:00]|9578 |\n",
      "|[2022-08-04 11:00:00, 2022-08-04 11:10:00]|20325|\n",
      "|[2022-08-04 11:20:00, 2022-08-04 11:30:00]|21411|\n",
      "|[2022-08-04 11:25:00, 2022-08-04 11:35:00]|21061|\n",
      "+------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    clear_output(wait=True)\n",
    "    spark.sql(\"SELECT * FROM sliding_window limit 10\").show(truncate=False)\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a8f15",
   "metadata": {},
   "source": [
    "## Average Speed Per Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3caefe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/28 04:22:48 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c62db688-e577-4c7a-aedc-cd1dfdccb16d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff7e8357e90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_traffic_stream\\\n",
    "    .withColumn('speed', F.col('vehicle_speed').cast('double'))\\\n",
    "    .groupBy(\n",
    "        F.window(\"DATE_TIME\", \"10 minutes\", \"5 minutes\"),\n",
    "        F.col('ADDRESS_ID')\n",
    "    )\\\n",
    "    .avg('speed')\\\n",
    "    .sort('window', 'address_id')\\\n",
    "    .writeStream\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"avg_speed_per_addr\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36fa1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------+------------------+\n",
      "|window                                    |ADDRESS_ID|avg(speed)        |\n",
      "+------------------------------------------+----------+------------------+\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|374       |51.8252427184466  |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|375       |36.703539823008846|\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|380       |50.104166666666664|\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|381       |44.356521739130436|\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|383       |52.02229299363057 |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|385       |49.11013215859031 |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|386       |45.54455445544554 |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|389       |46.425            |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|390       |42.301587301587304|\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|391       |37.67914438502674 |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|392       |45.72631578947368 |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|393       |45.035532994923855|\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|395       |32.239700374531836|\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|396       |42.118721461187214|\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|397       |45.13461538461539 |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|398       |51.62217659137577 |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|401       |32.09848484848485 |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|402       |43.81818181818182 |\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|403       |46.243852459016395|\n",
      "|[2022-08-04 11:05:00, 2022-08-04 11:15:00]|405       |44.091145833333336|\n",
      "+------------------------------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    clear_output(wait=True)\n",
    "    # remember to limit number of output rows\n",
    "    spark.sql(\"SELECT * FROM avg_speed_per_addr limit 20\").show(truncate=False)\n",
    "    sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
