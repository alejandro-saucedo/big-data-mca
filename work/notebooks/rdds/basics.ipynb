{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a62ebd1",
   "metadata": {},
   "source": [
    "# Create RDD\n",
    "One of the easiest ways to get RDDs is from an existing DataFrame or Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a0d517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.range(500).rdd\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57ad6b",
   "metadata": {},
   "source": [
    "However, by default, records are of type Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f24a968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records: [Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n",
      "record type: <class 'pyspark.sql.types.Row'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "records = rdd1.take(5)\n",
    "print(f'records: {records}')\n",
    "print(f'record type: {type(records[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe2e4a",
   "metadata": {},
   "source": [
    "So it probably needs to be mapped to the correct data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551732d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(500).rdd.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba697a",
   "metadata": {},
   "source": [
    "# Create an RDD from a local collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375d20b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[14] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = '''Please could you stop the noise?\n",
    "I'm trying to get some rest\n",
    "From all the unborn chicken\n",
    "Voices in my head\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "When I am king\n",
    "You will be first against the wall\n",
    "With your opinion\n",
    "Which is of no consequence at all'''.replace('\\n', ' ').split(' ')\n",
    "words = spark.sparkContext.parallelize(myCollection, 2) # the seconds parameter specifies the number of partitions\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29636e5c",
   "metadata": {},
   "source": [
    "# Read file using SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79850d53",
   "metadata": {},
   "source": [
    "Reading individual lines from text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf7e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78011"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = spark.sparkContext.textFile(\"/work/data/covid-tweets.json\")\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bb860",
   "metadata": {},
   "source": [
    "Reading full files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54244e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_files = spark.sparkContext.wholeTextFiles(\"/work/data/covid-tweets\")\n",
    "tweet_files.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe34ad",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52277861",
   "metadata": {},
   "source": [
    "### distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9b15ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', 'stop', 'noise?', 'trying', 'rest', 'in', 'head', \"What's\", 'may', 'but', 'an', 'android)', 'When', 'am', 'king', 'against', 'opinion', 'Which', 'is', 'of', 'no', 'at', 'could', 'you', 'the', \"I'm\", 'to', 'get', 'some', 'From', 'all', 'unborn', 'chicken', 'Voices', 'my', 'that?', '(I', 'be', 'paranoid,', 'not', 'I', 'You', 'will', 'first', 'wall', 'With', 'your', 'consequence']\n",
      "count: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "d_words = words.distinct()\n",
    "print(d_words.collect())\n",
    "print(f'count: {d_words.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910885b3",
   "metadata": {},
   "source": [
    "### filter\n",
    "select a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81522ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all', 'an', 'android)', 'an', 'android)', 'am', 'against', 'at', 'all']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFilterFunc(word):\n",
    "    # filter out each word not starting with 's'\n",
    "    return word.lower().startswith('a')\n",
    "\n",
    "f_words = words.filter(myFilterFunc)\n",
    "f_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0709dad",
   "metadata": {},
   "source": [
    "### map\n",
    "one-to-one transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8233924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'please'), ('c', 'could'), ('y', 'you'), ('s', 'stop'), ('t', 'the')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myMapFunc(word):\n",
    "    # map word to a tuple of a lower case first letter and word\n",
    "    word = word.lower()\n",
    "    return word[0], word\n",
    "m_words = words.map(myMapFunc)\n",
    "m_words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefad98b",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "one-to-many transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b19664a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial number of records: 62\n",
      "some letters:['p', 'l', 'e', 'a', 's', 'e', 'c', 'o', 'u', 'l', 'd', 'y', 'o', 'u', 's', 't', 'o', 'p', 't', 'h']\n",
      "final number of records: 253\n"
     ]
    }
   ],
   "source": [
    "print(f'initial number of records: {words.count()}')\n",
    "letters = words.flatMap(lambda word: list(word.lower())) # map a word to a series of lower case letters\n",
    "print(f'some letters:{letters.take(20)}')\n",
    "print(f'final number of records: {letters.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4537f",
   "metadata": {},
   "source": [
    "### sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683d7925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consequence',\n",
       " 'paranoid,',\n",
       " 'paranoid,',\n",
       " 'android)',\n",
       " 'android)',\n",
       " 'chicken',\n",
       " 'against',\n",
       " 'opinion',\n",
       " 'Please',\n",
       " 'noise?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by word length in descending length order\n",
    "s_words = words.sortBy(lambda word: len(word) * -1)\n",
    "s_words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70593d43",
   "metadata": {},
   "source": [
    "### randomSplit\n",
    "Create a given number of RDDs containing a number of elements based on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7372df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 12\n",
      "1: 50\n"
     ]
    }
   ],
   "source": [
    "split_rdds = words.randomSplit([0.2, 0.8]) # two RDDs with different sizes based on the given weights\n",
    "for i, split_rdd in enumerate(split_rdds):\n",
    "    print(f'{i}: {split_rdd.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a762b1",
   "metadata": {},
   "source": [
    "## Actions\n",
    "Actions either collect data to the driver or write to an external data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a17fb0",
   "metadata": {},
   "source": [
    "### reduce\n",
    "“reduce” an RDD of any kind of value to one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c04f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e1657",
   "metadata": {},
   "source": [
    "Example: get the longest word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8499f40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'consequence'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordLengthReducer(leftWord, rightWord):\n",
    "  if len(leftWord) > len(rightWord):\n",
    "    return leftWord\n",
    "  return rightWord\n",
    "\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e75beb4",
   "metadata": {},
   "source": [
    "### count, collect and take\n",
    "Too many examples by now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947a9cd",
   "metadata": {},
   "source": [
    "### countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fde424e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Please': 1,\n",
       "             'could': 1,\n",
       "             'you': 1,\n",
       "             'stop': 1,\n",
       "             'the': 3,\n",
       "             'noise?': 1,\n",
       "             \"I'm\": 1,\n",
       "             'trying': 1,\n",
       "             'to': 1,\n",
       "             'get': 1,\n",
       "             'some': 1,\n",
       "             'rest': 1,\n",
       "             'From': 1,\n",
       "             'all': 2,\n",
       "             'unborn': 1,\n",
       "             'chicken': 1,\n",
       "             'Voices': 1,\n",
       "             'in': 1,\n",
       "             'my': 1,\n",
       "             'head': 1,\n",
       "             \"What's\": 2,\n",
       "             'that?': 2,\n",
       "             '(I': 2,\n",
       "             'may': 2,\n",
       "             'be': 3,\n",
       "             'paranoid,': 2,\n",
       "             'but': 2,\n",
       "             'not': 2,\n",
       "             'an': 2,\n",
       "             'android)': 2,\n",
       "             'When': 1,\n",
       "             'I': 1,\n",
       "             'am': 1,\n",
       "             'king': 1,\n",
       "             'You': 1,\n",
       "             'will': 1,\n",
       "             'first': 1,\n",
       "             'against': 1,\n",
       "             'wall': 1,\n",
       "             'With': 1,\n",
       "             'your': 1,\n",
       "             'opinion': 1,\n",
       "             'Which': 1,\n",
       "             'is': 1,\n",
       "             'of': 1,\n",
       "             'no': 1,\n",
       "             'consequence': 1,\n",
       "             'at': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf640c",
   "metadata": {},
   "source": [
    "### first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fb30ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66fb3a",
   "metadata": {},
   "source": [
    "### takeOrdered, top and takeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9dffee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take(5):  ['Please', 'could', 'you', 'stop', 'the']\n",
      "takeOrdered:  ['(I', '(I', 'From', 'I', \"I'm\"]\n",
      "top(5):  ['your', 'you', 'will', 'wall', 'unborn']\n",
      "takeSample:  ['trying', \"What's\", 'first', 'but', 'rest', 'could']\n"
     ]
    }
   ],
   "source": [
    "print('take(5): ', words.take(5))\n",
    "print('takeOrdered: ', words.takeOrdered(5))\n",
    "print('top(5): ', words.top(5))\n",
    "withReplacement = True\n",
    "numberToTake = 6\n",
    "randomSeed = 100\n",
    "print('takeSample: ', words.takeSample(withReplacement, numberToTake, randomSeed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfbbf6c",
   "metadata": {},
   "source": [
    "## Saving Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76eeb94",
   "metadata": {},
   "source": [
    "### saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9bee6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.saveAsTextFile(\"file:/data/out/paranoid_android3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febdec3",
   "metadata": {},
   "source": [
    "### saveAsObjectFile\n",
    "Saves the file as a Hadoop sequence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7807c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sequence file consist of key-value pairs, so words first is mapped to that format\n",
    "words.map(lambda x: (None, x)).saveAsSequenceFile(\"/data/out/paranoid_android_seqfile2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd9819",
   "metadata": {},
   "source": [
    "## Caching and Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46a9d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_words = words.map(lambda w: w.lower()).cache()\n",
    "lower_words.count()\n",
    "# lower_words is cached and filter will work on the cached results\n",
    "lower_words.filter(lambda w: w.startswith('a')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85722c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointing in an HDFS path\n",
    "spark.sparkContext.setCheckpointDir(\"hdfs://namenode:9000/checkpoint\")\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb3f1e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(100000000)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8220cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
