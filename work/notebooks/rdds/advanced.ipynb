{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4c9b5e",
   "metadata": {},
   "source": [
    "# RDD Advanced Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c980d694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = '''Please could you stop the noise?\n",
    "I'm trying to get some rest\n",
    "From all the unborn chicken\n",
    "Voices in my head\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "When I am king\n",
    "You will be first against the wall\n",
    "With your opinion\n",
    "Which is of no consequence at all'''.replace('\\n', ' ').split(' ')\n",
    "words = spark.sparkContext.parallelize(myCollection, 3) # the second parameter specifies the number of partitions\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9e9b9",
   "metadata": {},
   "source": [
    "## Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd73b2d",
   "metadata": {},
   "source": [
    "### mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afee885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/19 00:33:55 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/11/19 00:34:10 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/11/19 00:34:25 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"partition: 0 => Please could you stop the noise? I'm trying to get some rest From all the unborn chicken Voices in my\",\n",
       " \"partition: 1 => head What's that? (I may be paranoid, but not an android) What's that? (I may be paranoid, but not an\",\n",
       " 'partition: 2 => android) When I am king You will be first against the wall With your opinion Which is of no consequence at all']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_by_partition(partition_index, records):\n",
    "    words_in_part = ' '.join(records)\n",
    "    return [f'partition: {partition_index} => {words_in_part}']\n",
    "words.mapPartitionsWithIndex(text_by_partition).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51d1ee",
   "metadata": {},
   "source": [
    "### foreachPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1029891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "out_path='/work/data/out/foreachpartition'\n",
    "if os.path.exists(out_path):\n",
    "    shutil.rmtree(out_path)\n",
    "os.makedirs(out_path)\n",
    "def write_text_by_parition(records):\n",
    "    import random\n",
    "    file_name = f'{out_path}/{str(random.random())[2:]}.txt'\n",
    "    with open(file_name, 'w') as file:\n",
    "        for word in records:\n",
    "            file.write(f'{word}\\n')\n",
    "        \n",
    "\n",
    "words.foreachPartition(write_text_by_parition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710c8e4",
   "metadata": {},
   "source": [
    "## Key-Value RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148246e3",
   "metadata": {},
   "source": [
    "### Creating a KV RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaa37037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('please', 1), ('could', 1), ('you', 1), ('stop', 1), ('the', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: (word.lower(), 1)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb13d0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'Please'), ('c', 'could'), ('y', 'you'), ('s', 'stop'), ('t', 'the')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword = words.keyBy(lambda word: word.lower()[0])\n",
    "keyword.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2735e",
   "metadata": {},
   "source": [
    "### Mapping over values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56f549d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'PLEASE'), ('c', 'COULD'), ('y', 'YOU'), ('s', 'STOP'), ('t', 'THE')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.mapValues(lambda word: word.upper()).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89e95ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('w', 'H'),\n",
       " ('t', 'H'),\n",
       " ('c', 'E'),\n",
       " ('y', 'R'),\n",
       " ('w', 'A'),\n",
       " ('t', 'T'),\n",
       " ('t', 'Y'),\n",
       " ('a', 'L'),\n",
       " ('p', 'A'),\n",
       " ('p', 'E')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.flatMapValues(lambda word: word.upper()).takeSample(True, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415542d5",
   "metadata": {},
   "source": [
    "### Extracting keys and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0692995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'c', 'y', 's', 't']\n",
      "['Please', 'could', 'you', 'stop', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(keyword.keys().take(5))\n",
    "print(keyword.values().take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64faac56",
   "metadata": {},
   "source": [
    "### lookup\n",
    "Note that there is no enforcement mechanism with respect to there being only one key for each input, so if we lookup “s”, we are going to get both values associated with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "561a78e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stop', 'some']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.lookup(\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313c6fe",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b491cb",
   "metadata": {},
   "source": [
    "### countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30939b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'p': 5,\n",
       "             'l': 10,\n",
       "             'e': 19,\n",
       "             'a': 23,\n",
       "             's': 12,\n",
       "             'c': 7,\n",
       "             'o': 22,\n",
       "             'u': 8,\n",
       "             'd': 8,\n",
       "             'y': 7,\n",
       "             't': 22,\n",
       "             'h': 13,\n",
       "             'n': 22,\n",
       "             'i': 22,\n",
       "             '?': 3,\n",
       "             \"'\": 3,\n",
       "             'm': 7,\n",
       "             'r': 10,\n",
       "             'g': 4,\n",
       "             'f': 3,\n",
       "             'b': 6,\n",
       "             'k': 2,\n",
       "             'v': 1,\n",
       "             'w': 7,\n",
       "             '(': 2,\n",
       "             ',': 2,\n",
       "             ')': 2,\n",
       "             'q': 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = words.flatMap(lambda word: word.lower())\n",
    "kv_chars = chars.map(lambda letter: (letter, 1))\n",
    "kv_chars.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13241810",
   "metadata": {},
   "source": [
    "### groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ab00e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('l', <pyspark.resultiterable.ResultIterable at 0x7f3702091150>),\n",
       " ('s', <pyspark.resultiterable.ResultIterable at 0x7f37020912d0>),\n",
       " ('o', <pyspark.resultiterable.ResultIterable at 0x7f3702091f90>),\n",
       " ('u', <pyspark.resultiterable.ResultIterable at 0x7f3702091fd0>),\n",
       " ('d', <pyspark.resultiterable.ResultIterable at 0x7f3702091310>)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_chars.groupByKey().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cdb162",
   "metadata": {},
   "source": [
    "Now sum up all the records in a group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "860ec08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('l', 10, pyspark.resultiterable.ResultIterable),\n",
       " ('s', 12, pyspark.resultiterable.ResultIterable),\n",
       " ('o', 22, pyspark.resultiterable.ResultIterable),\n",
       " ('u', 8, pyspark.resultiterable.ResultIterable),\n",
       " ('d', 8, pyspark.resultiterable.ResultIterable)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_key_values(row):\n",
    "    key = row[0]\n",
    "    values = row[1]\n",
    "    total = sum(v for v in values)\n",
    "    # note that values are collected in memory, which may result in an memory error when there are too many values for a key\n",
    "    return key, total, type(values)\n",
    "kv_chars.groupByKey().map(sum_key_values).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f2cc1",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "A safe alternative to summing using groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd4f8247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('l', 10),\n",
       " ('s', 12),\n",
       " ('o', 22),\n",
       " ('u', 8),\n",
       " ('d', 8),\n",
       " ('t', 22),\n",
       " ('n', 22),\n",
       " ('?', 3),\n",
       " (\"'\", 3),\n",
       " ('k', 2)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groups of values are not loaded into memory\n",
    "kv_chars.reduceByKey(lambda left_value, right_value: left_value + right_value ).take(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2d5e8",
   "metadata": {},
   "source": [
    "### aggregate\n",
    "This function requires a null and start value. \n",
    "It operates in two levels. The first aggregates within partitions, the second aggregates across partitions. \n",
    "The start value will be used at both aggregation levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "573a4057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greatest values per partition: [5, 10, 15, 20, 25, 30]\n",
      "summation of greatest values per partition: 105\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize(range(1,31), 6)\n",
    "\n",
    "# sum the greatest values per partition\n",
    "\n",
    "def max_func(left, right):\n",
    "  return max(left, right)\n",
    "\n",
    "def add_func(left, right):\n",
    "  return left + right\n",
    "\n",
    "values_per_part = nums.mapPartitions(lambda records: [max(records)]).collect()\n",
    "print(f'greatest values per partition: {values_per_part}')\n",
    "\n",
    "aggr_result = nums.aggregate(0, max_func, add_func)\n",
    "\n",
    "print(f'summation of greatest values per partition: {aggr_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e630a9a9",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08251215",
   "metadata": {},
   "source": [
    "### cogroup\n",
    "groups together two key–value RDDs. \n",
    "This joins the given values by key. This is effectively just a group-based join on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c68f574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cogroup result:\n",
      "('l', (<pyspark.resultiterable.ResultIterable object at 0x7f37020f0610>, <pyspark.resultiterable.ResultIterable object at 0x7f37020f0a90>))\n",
      "('s', (<pyspark.resultiterable.ResultIterable object at 0x7f37020f0ed0>, <pyspark.resultiterable.ResultIterable object at 0x7f37020f02d0>))\n",
      "('d', (<pyspark.resultiterable.ResultIterable object at 0x7f37020a09d0>, <pyspark.resultiterable.ResultIterable object at 0x7f37020a0a10>))\n",
      "('?', (<pyspark.resultiterable.ResultIterable object at 0x7f37020f03d0>, <pyspark.resultiterable.ResultIterable object at 0x7f37020f08d0>))\n",
      "('v', (<pyspark.resultiterable.ResultIterable object at 0x7f3702093750>, <pyspark.resultiterable.ResultIterable object at 0x7f3702093590>))\n",
      "\n",
      "cogroup result interpreted:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"l: ['0.711438818967337', '0.3286241666392802']\",\n",
       " \"s: ['0.7431762051021537', '0.12156307110968012']\",\n",
       " \"d: ['0.26369260272781625', '0.6835099757656944']\",\n",
       " \"?: ['0.4272008619477561', '0.47953632197402096']\",\n",
       " \"v: ['0.7142809192237576', '0.06766917321999166']\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "distinct_chars = words.flatMap(lambda word: word.lower()).distinct()\n",
    "char_rdd_1 = distinct_chars.map(lambda c: (c, random.random()))\n",
    "char_rdd_2 = distinct_chars.map(lambda c: (c, random.random()))\n",
    "group = char_rdd_1.cogroup(char_rdd_2)\n",
    "print('cogroup result:')\n",
    "print(\"\\n\".join([str(r) for r in group.take(5)]))\n",
    "\n",
    "def pretty_output(row):\n",
    "    key = row[0]\n",
    "    groups = row[1] # groups consist of nested iterables\n",
    "    values = [str(v) for group in groups for v in group]\n",
    "    return f'{key}: {values}'\n",
    "print('\\ncogroup result interpreted:')\n",
    "group.map(pretty_output).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac6182",
   "metadata": {},
   "source": [
    "### innerJoin\n",
    "joins allow setting the number of output partitions.\n",
    "All join types (inner, fullOuter, leftOuter and rightOuter) follow the same pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67361696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 86:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 87:=======================================>                 (7 + 2) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kv_chars = words.distinct().map(lambda c: (c, random.random()))\n",
    "output_partitions = 10\n",
    "print(kv_chars.count())\n",
    "print(kv_chars.join(kv_chars).count())\n",
    "print(kv_chars.join(kv_chars, output_partitions).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204bd283",
   "metadata": {},
   "source": [
    "### zip\n",
    "Groups two rdds with the same number of partitions and cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c3ff037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 10),\n",
       " (1, 11),\n",
       " (2, 12),\n",
       " (3, 13),\n",
       " (4, 14),\n",
       " (5, 15),\n",
       " (6, 16),\n",
       " (7, 17),\n",
       " (8, 18),\n",
       " (9, 19)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range1 = sc.parallelize(range(10), 2)\n",
    "range2 = sc.parallelize(range(10, 20), 2)\n",
    "range1.zip(range2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752e830",
   "metadata": {},
   "source": [
    "## Controlling Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0bfab",
   "metadata": {},
   "source": [
    "### coalesce\n",
    "Collapses partitions, trying to reduce the amount of data moved across nodes. However, partitions may end up skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68054edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number partitions with size:  [20, 20, 22]\n",
      "partitions with size after coalesce:  [20, 42]\n"
     ]
    }
   ],
   "source": [
    "partsSizes = words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('original number partitions with size: ', partsSizes)\n",
    "# collapse from 3 to 2 partitions\n",
    "coal_words = words.coalesce(2)\n",
    "coal_partsSizes = coal_words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('partitions with size after coalesce: ', coal_partsSizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41191843",
   "metadata": {},
   "source": [
    "### repartition\n",
    "Allows a repartition in th data up or down but performs a shuffle across nodes in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb150210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number partitions with size:  [20, 20, 22]\n",
      "partitions with size after collapsing with repartition:  [32, 30]\n"
     ]
    }
   ],
   "source": [
    "partsSizes = words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('original number partitions with size: ', partsSizes)\n",
    "# collapse from 3 to 2 partitions\n",
    "collapsed_words = words.repartition(2)\n",
    "collapsed_partsSizes = collapsed_words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('partitions with size after collapsing with repartition: ', collapsed_partsSizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dedbfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitions with size after expanding with repartition:  [0, 10, 20, 20, 12]\n"
     ]
    }
   ],
   "source": [
    "exp_words = words.repartition(5)\n",
    "exp_partsSizes = exp_words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('partitions with size after expanding with repartition: ', exp_partsSizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82380e40",
   "metadata": {},
   "source": [
    "### Custom Partitioning\n",
    "\n",
    "Custom partitioning allows fine grained control over how data is distributed. It may be usefull to handle data skew or data movement.\n",
    "\n",
    "The following example loads a list of customers. There are two customer that need to be put in their own partitions are they may fill up the memory due to their number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e72835b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: \n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retail RDD partitions with size: [298310, 243599]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/work/data/online-retail-dataset.csv\")\n",
    "print('Schema: ')\n",
    "df.printSchema()\n",
    "# lets collapse to simulate data skew\n",
    "retail = df.coalesce(10).rdd\n",
    "partsSizes = retail.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('Retail RDD partitions with size:', partsSizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d4838",
   "metadata": {},
   "source": [
    "A partitioner will be used to put two specific customers in the same partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9cae7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retail RDD partitions with size: ['0: 563', '1: 180475', '2: 180459', '3: 180412']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# partitioner\n",
    "def partitioning_func(key):\n",
    "  import random\n",
    "  if key == 17850 or key == 12583:\n",
    "    # records which belong to these customer go to partition 0\n",
    "    return 0\n",
    "  else:\n",
    "    # any other customer's records go to whatever other partition number from 1 to 3\n",
    "    return random.randint(1,3)\n",
    "\n",
    "# convert retailt RDD to a key-value RDD using the customer number as key\n",
    "kv_retail = retail.keyBy(lambda row: row[6])\n",
    "\n",
    "repart_kv_retail = kv_retail.partitionBy(4, partitioning_func)\n",
    "\n",
    "partsSizes = repart_kv_retail.mapPartitionsWithIndex(lambda idx, records: [f'{idx}: {sum(1 for _ in records)}']).collect()\n",
    "print('Retail RDD partitions with size:', partsSizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422f72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
