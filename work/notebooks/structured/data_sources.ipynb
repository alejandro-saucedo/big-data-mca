{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b932da1c",
   "metadata": {},
   "source": [
    "# Data Source API\n",
    "\n",
    "Spark provides a flexible and powerful API for reading from different formats and sources.\n",
    "\n",
    "Here are some examples but there is support for much more formats and it is possible to create new data sources if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa291cb",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8005e",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "510cadf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"mode\", \"FAILFAST\") \\\n",
    "  .load(\"/work/data/flight-data/csv/2010-summary.csv\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d40ebc8",
   "metadata": {},
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41f45f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .save(\"/tmp/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effbfd3c",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0538da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1381bf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").option(\"mode\", \"FAILFAST\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"/work/data/flight-data/json/2010-summary.json\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338d695",
   "metadata": {},
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3936444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"json\").mode(\"overwrite\").save(\"/tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6146a",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a68243",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e72570ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optionally with format('parquet')\n",
    "df = spark.read.load(\"/work//data/flight-data/parquet/2010-summary.parquet\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5681c",
   "metadata": {},
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2f7841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").save(\"/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8060aa7a",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "For this example you need to edit the file master/jupyter.sh (in your host machine) and add the following options to the pyspark command:\n",
    "\n",
    "```--driver-class-path /work/lib/sqlite-jdbc-3.39.3.0.jar --jars /work/lib/sqlite-jdbc-3.39.3.0.jar```\n",
    "\n",
    "The previous options provide the path of the sqlite driver library.\n",
    "Once the changes have been applied, re run Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d92f1",
   "metadata": {},
   "source": [
    "Set data source options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfdd7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/work/data/flight-data/jdbc/my-sqlite.db\"\n",
    "url = \"jdbc:sqlite:\" + path\n",
    "driver = \"org.sqlite.JDBC\"\n",
    "tablename = \"flight_info\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fae033",
   "metadata": {},
   "source": [
    "Connect to data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7125a88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_dataframe = spark.read.format(\"jdbc\").\\\n",
    "        option(\"url\", url).\\\n",
    "        option(\"dbtable\", tablename).\\\n",
    "        option(\"driver\",  driver).\\\n",
    "        load()\n",
    "db_dataframe.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da93f87",
   "metadata": {},
   "source": [
    "Let's query the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c1bc7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|         Anguilla|\n",
      "|           Russia|\n",
      "|         Paraguay|\n",
      "+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qry_db_dataframe = db_dataframe.select('DEST_COUNTRY_NAME').distinct()\n",
    "qry_db_dataframe.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a56437",
   "metadata": {},
   "source": [
    "Wih JDBC/ODBC, Spark tries to push down predicates to the database query level, instead of filtering in the Spark side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3024e62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#25], functions=[])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#25, 200), true, [id=#125]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#25], functions=[])\n",
      "      +- *(1) Scan JDBCRelation(flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#25] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qry_db_dataframe.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "However, explicit database queries can be specified instead of table names at the loading time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5122929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdown_qry = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info\"\"\"\n",
    "db_dataframe = spark.read.format(\"jdbc\").\\\n",
    "        option(\"url\", url).\\\n",
    "        option(\"dbtable\", pushdown_qry).\\\n",
    "        option(\"driver\",  driver).\\\n",
    "        load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
