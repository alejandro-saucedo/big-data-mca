{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a7a039",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "This notebook will show some operation done previously using the DataFrame API but using SQL.\n",
    "\n",
    "It also shows some functionality specific to SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb6e61",
   "metadata": {},
   "source": [
    "## Creating a temporal view\n",
    "A temporal view or a table needs to be registered to work with SQL statments. It can be created from an existing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933edf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('/work//data/flight-data/json/2015-summary.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('flights_view')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f82d4",
   "metadata": {},
   "source": [
    "## First SQL statement\n",
    "Once a temporal view is registered, it can be used to perform SQL statements. The output of a SQL transformation is always a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ccd0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|total|\n",
      "+-----------------+-----+\n",
      "|         Anguilla|   41|\n",
      "|           Russia|  176|\n",
      "|         Paraguay|   60|\n",
      "|          Senegal|   40|\n",
      "|           Sweden|  118|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql('''\n",
    "                SELECT DEST_COUNTRY_NAME, sum(count) as total\n",
    "                FROM flights_view GROUP BY DEST_COUNTRY_NAME\n",
    "            ''')\n",
    "print(f'Result type: {type(df)}')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99b167",
   "metadata": {},
   "source": [
    "DataFrame API's transformation can be mixed with SQL as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7a50f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "            SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "            FROM flights_view GROUP BY DEST_COUNTRY_NAME\n",
    "        ''').\\\n",
    "    where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\").\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a21f40",
   "metadata": {},
   "source": [
    "## SQL statements\n",
    "\n",
    "```\n",
    "SELECT [ALL|DISTINCT] named_expression[, named_expression, ...]\n",
    "    FROM relation[, relation, ...]\n",
    "    [lateral_view[, lateral_view, ...]]\n",
    "    [WHERE boolean_expression]\n",
    "    [aggregation [HAVING boolean_expression]]\n",
    "    [ORDER BY sort_expressions]\n",
    "    [CLUSTER BY expressions]\n",
    "    [DISTRIBUTE BY expressions]\n",
    "    [SORT BY sort_expressions]\n",
    "    [WINDOW named_window[, WINDOW named_window, ...]]\n",
    "    [LIMIT num_rows]\n",
    "\n",
    "named_expression:\n",
    "    : expression [AS alias]\n",
    "\n",
    "relation:\n",
    "    | join_relation\n",
    "    | (table_name|query|relation) [sample] [AS alias]\n",
    "    : VALUES (expressions)[, (expressions), ...]\n",
    "          [AS (column_name[, column_name, ...])]\n",
    "\n",
    "expressions:\n",
    "    : expression[, expression, ...]\n",
    "\n",
    "sort_expressions:\n",
    "    : expression [ASC|DESC][, expression [ASC|DESC], ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b4ef2",
   "metadata": {},
   "source": [
    "Select all and a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b127aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "|            Egypt|      United States|   15|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\n",
    "    FROM flights_view''').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970198d9",
   "metadata": {},
   "source": [
    "Do some table-level aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a0e668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:====================================================> (196 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT avg(count), count(distinct(DEST_COUNTRY_NAME)) FROM flights_view''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d6739",
   "metadata": {},
   "source": [
    "Filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22be818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM flights_view WHERE count < 2 AND ORIGIN_COUNTRY_NAME != \"Croatia\"').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8450583",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c71f3e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|split(DEST_COUNTRY_NAME,  , -1)|\n",
      "+-------------------------------+\n",
      "|               [United, States]|\n",
      "|               [United, States]|\n",
      "|               [United, States]|\n",
      "|                        [Egypt]|\n",
      "|               [United, States]|\n",
      "+-------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT SPLIT(DEST_COUNTRY_NAME, ' ') FROM flights_view''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6507da",
   "metadata": {},
   "source": [
    "Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff52758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+--------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|exploded|\n",
      "+-----------------+-------------------+-----+--------+\n",
      "|    United States|            Romania|   15|  United|\n",
      "|    United States|            Romania|   15|  States|\n",
      "|    United States|            Croatia|    1|  United|\n",
      "|    United States|            Croatia|    1|  States|\n",
      "|    United States|            Ireland|  344|  United|\n",
      "|    United States|            Ireland|  344|  States|\n",
      "|            Egypt|      United States|   15|   Egypt|\n",
      "|    United States|              India|   62|  United|\n",
      "|    United States|              India|   62|  States|\n",
      "|    United States|          Singapore|    1|  United|\n",
      "+-----------------+-------------------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count, exploded\n",
    "    FROM (SELECT *, SPLIT(DEST_COUNTRY_NAME, ' ') as splitted FROM flights_view)\n",
    "    LATERAL VIEW explode(splitted) as exploded''').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9b2a9",
   "metadata": {},
   "source": [
    "Retrieve a certain number of results only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea6b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DEST_COUNTRY_NAME FROM flights_view LIMIT 2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd405c8c",
   "metadata": {},
   "source": [
    "Getting distinct results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec34b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|ORIGIN_COUNTRY_NAME|\n",
      "+-------------------+\n",
      "|           Paraguay|\n",
      "|             Russia|\n",
      "|           Anguilla|\n",
      "|            Senegal|\n",
      "|             Sweden|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT ORIGIN_COUNTRY_NAME FROM flights_view').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c3ac4",
   "metadata": {},
   "source": [
    "Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7065c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM flights_view ORDER BY count DESC, DEST_COUNTRY_NAME ASC').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31dc37",
   "metadata": {},
   "source": [
    "Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12365cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===========================================>          (160 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|ORIGIN_COUNTRY_NAME| total|\n",
      "+-------------------+------+\n",
      "|      United States|411966|\n",
      "|             Canada|  8483|\n",
      "|             Mexico|  7187|\n",
      "|     United Kingdom|  1970|\n",
      "|              Japan|  1496|\n",
      "| Dominican Republic|  1420|\n",
      "|            Germany|  1336|\n",
      "|        The Bahamas|   986|\n",
      "|             France|   952|\n",
      "|              China|   920|\n",
      "|           Colombia|   867|\n",
      "|        South Korea|   827|\n",
      "|            Jamaica|   712|\n",
      "|        Netherlands|   660|\n",
      "|             Brazil|   619|\n",
      "|         Costa Rica|   608|\n",
      "|        El Salvador|   508|\n",
      "|               Cuba|   478|\n",
      "|             Panama|   465|\n",
      "|              Spain|   442|\n",
      "+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:=================================================>    (182 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT ORIGIN_COUNTRY_NAME, sum(count) AS total \n",
    "    FROM flights_view \n",
    "    GROUP BY ORIGIN_COUNTRY_NAME \n",
    "    ORDER BY total DESC''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf15b4",
   "metadata": {},
   "source": [
    "Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8542002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new table to join with\n",
    "spark.createDataFrame([\n",
    "    ('United States', \"New York\"),\n",
    "    ('United States', \"San Francisco\"),\n",
    "    ('Mexico', \"Guadalajara\"),\n",
    "    ('Canda', 'Toronto')])\\\n",
    "  .toDF(\"COUNTRY\", \"CITY\").createTempView('airports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7416c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|      COUNTRY|         CITY|\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "|       United States|             Mexico| 7187|       Mexico|  Guadalajara|\n",
      "|              Belize|      United States|  188|United States|San Francisco|\n",
      "|Bonaire, Sint Eus...|      United States|   58|United States|San Francisco|\n",
      "|             Uruguay|      United States|   43|United States|San Francisco|\n",
      "|            Bulgaria|      United States|    3|United States|San Francisco|\n",
      "|        Cook Islands|      United States|   13|United States|San Francisco|\n",
      "|           Australia|      United States|  329|United States|San Francisco|\n",
      "|               Chile|      United States|  174|United States|San Francisco|\n",
      "|           Greenland|      United States|    2|United States|San Francisco|\n",
      "|             Nigeria|      United States|   59|United States|San Francisco|\n",
      "|              Poland|      United States|   32|United States|San Francisco|\n",
      "|              France|      United States|  935|United States|San Francisco|\n",
      "|          Guadeloupe|      United States|   56|United States|San Francisco|\n",
      "|       New Caledonia|      United States|    1|United States|San Francisco|\n",
      "|               Ghana|      United States|   18|United States|San Francisco|\n",
      "|              Angola|      United States|   15|United States|San Francisco|\n",
      "|         South Korea|      United States| 1048|United States|San Francisco|\n",
      "|               Palau|      United States|   30|United States|San Francisco|\n",
      "|              Jordan|      United States|   44|United States|San Francisco|\n",
      "|             Austria|      United States|   62|United States|San Francisco|\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:==============>                                           (1 + 3) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT * \n",
    "    FROM flights_view f \n",
    "    JOIN airports a\n",
    "    ON f.ORIGIN_COUNTRY_NAME=a.COUNTRY\n",
    "    ORDER BY a.COUNTRY''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e23d56",
   "metadata": {},
   "source": [
    "Case-When"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d55d84b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|condition|\n",
      "+---------+\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|        0|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "|       -1|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT\n",
    "        (CASE \n",
    "            WHEN DEST_COUNTRY_NAME = 'UNITED STATES' THEN 1\n",
    "            WHEN DEST_COUNTRY_NAME = 'Egypt' THEN 0\n",
    "            ELSE -1 \n",
    "        END) as condition\n",
    "    FROM flights_view\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f356509c",
   "metadata": {},
   "source": [
    "## Data Definition Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6b660",
   "metadata": {},
   "source": [
    "### Drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcadea",
   "metadata": {},
   "source": [
    "Dropping a view (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79bdb1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/13 17:55:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/05/13 17:55:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/05/13 17:55:06 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/05/13 17:55:06 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.20.0.7\n",
      "23/05/13 17:55:07 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP VIEW IF EXISTS flights_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7466b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the table again to re use it later\n",
    "spark.read.json('/work//data/flight-data/json/2015-summary.json').\\\n",
    "    createOrReplaceTempView('flights_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f8419",
   "metadata": {},
   "source": [
    "The DataFrame is not dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "586a63f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|DEST_COUNTRY_NAME|total|\n",
      "+-----------------+-----+\n",
      "|         Anguilla|   41|\n",
      "|           Russia|  176|\n",
      "|         Paraguay|   60|\n",
      "|          Senegal|   40|\n",
      "|           Sweden|  118|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1addda",
   "metadata": {},
   "source": [
    "Droping a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a73cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS flights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44c2c5",
   "metadata": {},
   "source": [
    "### Creating a table\n",
    "\n",
    "A table lives across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6feab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/13 17:55:10 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider JSON. Persisting data source table `default`.`flights` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "23/05/13 17:55:10 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/05/13 17:55:11 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/05/13 17:55:11 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/05/13 17:55:11 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "        CREATE TABLE flights (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG) \n",
    "        USING JSON OPTIONS (path '/work/data/flight-data/json/2015-summary.json')\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "868702e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM flights').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a3d6f",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64a71a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CACHE TABLE flights_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b078d",
   "metadata": {},
   "source": [
    "And uncaching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9cfb079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('UNCACHE TABLE flights_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37d3b9",
   "metadata": {},
   "source": [
    "Or caching lazily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4c08a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CACHE LAZY TABLE flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95bda4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('SELECT * from flights').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f964c6",
   "metadata": {},
   "source": [
    "### Show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2a41ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "| default|            flights|      false|\n",
      "| default|           flights1|      false|\n",
      "| default|       hive_flights|      false|\n",
      "| default|just_usa_view_temp4|      false|\n",
      "|        |           airports|       true|\n",
      "|        |       flights_view|       true|\n",
      "+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98aece",
   "metadata": {},
   "source": [
    "### Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "981543fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/13 17:55:13 WARN ObjectStore: Failed to get database new_db, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE new_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288c2c6",
   "metadata": {},
   "source": [
    "Show the available databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63210898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   new_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW DATABASES').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60168e83",
   "metadata": {},
   "source": [
    "Which database is being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "368c6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT current_database()').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c799010",
   "metadata": {},
   "source": [
    "Switch to use another database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "705c8da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE new_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81d4e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|            new_db|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT current_database()').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecac0c6",
   "metadata": {},
   "source": [
    "Drop a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c57ff9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/13 17:55:13 WARN TxnHandler: Cannot perform cleanup since metastore table does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP DATABASE IF EXISTS new_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0586b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
