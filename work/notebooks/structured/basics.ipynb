{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0fc540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"/data/flight-data-2015-summary.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c5bca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59964b09",
   "metadata": {},
   "source": [
    "## Schemas\n",
    "\n",
    "A schema defines the column names and types of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7ebc232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95162c14",
   "metadata": {},
   "source": [
    "A schema is a StructType made up of a number of fields, StructFields, that have a name, type, a Boolean flag which specifies whether that column can contain missing or null values, and, finally, users can optionally specify associated metadata with that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3bfff064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61f1f2",
   "metadata": {},
   "source": [
    "Creating a schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d4f6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "manual_schema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c2b0d",
   "metadata": {},
   "source": [
    "Creating a dataframe using the created schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed2b8893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").schema(manual_schema).load(\"/data/flight-data-2015-summary.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcaea2f",
   "metadata": {},
   "source": [
    "## Columns and expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa9fb0",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29343092",
   "metadata": {},
   "source": [
    "Columns are expressed using the col and columns functions (interchangeably)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d3463b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'someColumnName'>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900e37e",
   "metadata": {},
   "source": [
    "Columns are referenced from dataframes using the indexing operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7fbdf6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'count'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd4f77",
   "metadata": {},
   "source": [
    "Getting the list of column names from a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cfc46a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f4725",
   "metadata": {},
   "source": [
    "### Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f99ab",
   "metadata": {},
   "source": [
    "Expression is a more general concept wich involves columns and columns transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7d8bf",
   "metadata": {},
   "source": [
    "Different ways of creating expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e7642bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'(someCol - 5)'>\n",
      "Column<b'(someCol - 5)'>\n",
      "Column<b'(someCol - 5)'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "print(expr(\"someCol - 5\"))\n",
    "print(col(\"someCol\") - 5)\n",
    "print(expr(\"someCol\") - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a203991",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Records and rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350bcb7",
   "metadata": {},
   "source": [
    "1- In Spark, each row in a DataFrame is a single record. \n",
    "\n",
    "2- Spark represents this record as an object of type Row. \n",
    "\n",
    "3- Spark manipulates Row objects using column expressions in order to produce usable values. \n",
    "\n",
    "4- Row objects internally represent arrays of bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed14505",
   "metadata": {},
   "source": [
    "Getting the first row from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22777718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "field: United States, type: <class 'str'>\n",
      "field: Romania, type: <class 'str'>\n",
      "field: 15, type: <class 'int'>\n",
      "Value in the count column:  15\n",
      "Value in the second column:  Romania\n"
     ]
    }
   ],
   "source": [
    "row = df.first()\n",
    "print(row)\n",
    "# Iterating through the row's valus\n",
    "for field in row:\n",
    "    print(f'field: {field}, type: {type(field)}')\n",
    "# accessing values randomly\n",
    "print('Value in the count column: ', row['count'])\n",
    "print('Value in the second column: ', row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506c9a0",
   "metadata": {},
   "source": [
    "Creating a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6d83474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row('Hello', None, 1, False)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "my_row = Row(\"Hello\", None, 1, False)\n",
    "my_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25468b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2fb833",
   "metadata": {},
   "source": [
    "Types of transformations:\n",
    "\n",
    " 1- add rows or columns\n",
    "\n",
    " 2- remove rows or columns\n",
    "\n",
    " 3- transform a row into a column (or vice versa)\n",
    "\n",
    " 4- change the order of rows based on the values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc98c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0b0b2",
   "metadata": {},
   "source": [
    "The easy way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06c2c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 49:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"/data/flight-data-2015-summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9e1b2",
   "metadata": {},
   "source": [
    "The manual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5941795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 50:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "manual_schema = StructType([\n",
    "  StructField(\"some\", StringType(), True),\n",
    "  StructField(\"col\", StringType(), True),\n",
    "  StructField(\"names\", LongType(), False)\n",
    "])\n",
    "some_row = Row(\"Hello\", None, 1)\n",
    "manual_df = spark.createDataFrame([some_row], manual_schema)\n",
    "manual_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97cfc7",
   "metadata": {},
   "source": [
    "### select and selectExpr\n",
    "\n",
    "Equivalent to the select part of an SQL query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bdfb6d",
   "metadata": {},
   "source": [
    "Selecting a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db9978a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3c70d",
   "metadata": {},
   "source": [
    "Selecting multiple columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "be2ce86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c2fcf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    "    expr(\"DEST_COUNTRY_NAME\"),\n",
    "    col(\"DEST_COUNTRY_NAME\"),\n",
    "    column(\"DEST_COUNTRY_NAME\"))\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dccfde",
   "metadata": {},
   "source": [
    "Selecting using expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c7d60a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ba2e3",
   "metadata": {},
   "source": [
    "selectExpr is a shorthand to having to specidy expr or col:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea19d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0cc29733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "  \"*\", # all original columns\n",
    "  \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\" # adding a new boolean column\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ded3a",
   "metadata": {},
   "source": [
    "Using aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0149d22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:====================================================> (193 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3eea50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9dcb9a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "# adding a column of literals\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c6430386",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d9fc5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding a literal column again\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff624303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or a more complex expression\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574be02",
   "metadata": {},
   "source": [
    "### Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75f320a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "72bedc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Column names with spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d567467",
   "metadata": {},
   "source": [
    "Let's add one column name with spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "544d1859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania|   15|              Romania|\n",
      "|    United States|            Croatia|    1|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_space_col = df.withColumn(\n",
    "    \"This Long Column-Name\", # nothing special here as the expected type of the parameter is str\n",
    "    expr(\"ORIGIN_COUNTRY_NAME\"))\n",
    "df_with_space_col.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b21d9a",
   "metadata": {},
   "source": [
    "Columns with spaces must be enclosed with \\`\\` when used in expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c95ba6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_space_col.selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` as `new col`\")\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76dbae2",
   "metadata": {},
   "source": [
    "### Removing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d2c99b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124e4b2",
   "metadata": {},
   "source": [
    "### Changing column's type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "035100b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: string]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"count2\", col(\"count\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120c466",
   "metadata": {},
   "source": [
    "## Filtering rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790d28d",
   "metadata": {},
   "source": [
    "Using filter or where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1e28377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\") < 2).show(2)\n",
    "df.where(\"count < 2\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f91c43",
   "metadata": {},
   "source": [
    "Chainning filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "52def936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade94f2d",
   "metadata": {},
   "source": [
    "### Getting unique rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ffbcae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "06464b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "15fed566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "with_replacement = False\n",
    "fraction = 0.3\n",
    "df.sample(with_replacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2b17f",
   "metadata": {},
   "source": [
    "### Appending rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "64778ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new dataframe\n",
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "rows = [\n",
    "  Row(\"New Country\", \"Other Country\", 5),\n",
    "  Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "rdd = spark.sparkContext.parallelize(rows)\n",
    "another_df = spark.createDataFrame(rows, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364ddb4",
   "metadata": {},
   "source": [
    "Performing the union:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "24b04e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Bonaire, Sint Eustatius, and Saba', ORIGIN_COUNTRY_NAME='United States', count=58),\n",
       " Row(DEST_COUNTRY_NAME='Greece', ORIGIN_COUNTRY_NAME='United States', count=30),\n",
       " Row(DEST_COUNTRY_NAME='New Country', ORIGIN_COUNTRY_NAME='Other Country', count=5),\n",
       " Row(DEST_COUNTRY_NAME='New Country 2', ORIGIN_COUNTRY_NAME='Other Country 3', count=1)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.union(another_df).tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828acd99",
   "metadata": {},
   "source": [
    "### Sorting rows\n",
    "Using sort or orderBy\n",
    "Always sort in ascending order by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "339df5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.sort(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f7c7e",
   "metadata": {},
   "source": [
    "With count in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e1a6eb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\").desc(), \"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aebb50",
   "metadata": {},
   "source": [
    "## Repartition and Coalesce\n",
    "\n",
    "Work exactly the same as with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2ce9b7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22aab1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef5704",
   "metadata": {},
   "source": [
    "### Repartition by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b19d25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions after repartition: MapPartitionsRDD[348] at javaToPython at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "repart_rdd = df.repartition(5, col(\"DEST_COUNTRY_NAME\")).rdd\n",
    "print('Number of partitions after repartition:', repart_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35022dfe",
   "metadata": {},
   "source": [
    "### Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "95b80da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b09e96",
   "metadata": {},
   "source": [
    "### Replacing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "81290c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# loading a different data set\n",
    "retail = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/data/online-retail-dataset.csv\")\n",
    "retail.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dc24f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f7eb6",
   "metadata": {},
   "source": [
    "Remove all rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83c60d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 541909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 83:>                                                         (0 + 4) / 4]\r",
      "\r",
      "[Stage 83:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After:  406829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('Before:', retail.count())\n",
    "print('After: ', retail.na.drop().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222a59c",
   "metadata": {},
   "source": [
    "Or applying na.drop only to a subset of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7409f5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 85:=============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "406829"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail.na.drop(subset=[\"CustomerID\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bd015e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4078306f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/30 23:46:11 ERROR TaskSchedulerImpl: Lost executor 0 on 172.20.0.8: worker lost\n",
      "22/10/30 23:46:12 WARN TransportChannelHandler: Exception in connection from /172.20.0.8:46572\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:377)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "retail.na.fill(0).drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563dda8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
