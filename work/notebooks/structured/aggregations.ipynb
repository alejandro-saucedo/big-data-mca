{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e909a2",
   "metadata": {},
   "source": [
    "# Aggregations\n",
    "\n",
    "This notebook shows some basic aggregation methods provided by Spark.\n",
    "\n",
    "More comples aggregations types (windows an cubes) exist but are not covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e58e358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 134:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "22/11/06 23:40:56 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/work/data/retail-data/all/*.csv\")\\\n",
    "  .coalesce(5)\n",
    "df.cache()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e64bbd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " summary     | count                \n",
      " InvoiceNo   | 541909               \n",
      " StockCode   | 541909               \n",
      " Description | 540455               \n",
      " Quantity    | 541909               \n",
      " InvoiceDate | 541909               \n",
      " UnitPrice   | 541909               \n",
      " CustomerID  | 406829               \n",
      " Country     | 541909               \n",
      "-RECORD 1---------------------------\n",
      " summary     | mean                 \n",
      " InvoiceNo   | 559965.752026781     \n",
      " StockCode   | 27623.240210938104   \n",
      " Description | 20713.0              \n",
      " Quantity    | 9.55224954743324     \n",
      " InvoiceDate | null                 \n",
      " UnitPrice   | 4.611113626082961    \n",
      " CustomerID  | 15287.690570239585   \n",
      " Country     | null                 \n",
      "-RECORD 2---------------------------\n",
      " summary     | stddev               \n",
      " InvoiceNo   | 13428.41728080439    \n",
      " StockCode   | 16799.737628427712   \n",
      " Description | NaN                  \n",
      " Quantity    | 218.08115785023486   \n",
      " InvoiceDate | null                 \n",
      " UnitPrice   | 96.75985306117956    \n",
      " CustomerID  | 1713.600303321589    \n",
      " Country     | null                 \n",
      "-RECORD 3---------------------------\n",
      " summary     | min                  \n",
      " InvoiceNo   | 536365               \n",
      " StockCode   | 10002                \n",
      " Description |  4 PURPLE FLOCK D... \n",
      " Quantity    | -80995               \n",
      " InvoiceDate | 1/10/2011 10:04      \n",
      " UnitPrice   | -11062.06            \n",
      " CustomerID  | 12346                \n",
      " Country     | Australia            \n",
      "-RECORD 4---------------------------\n",
      " summary     | 25%                  \n",
      " InvoiceNo   | 547903.0             \n",
      " StockCode   | 21929.0              \n",
      " Description | 20713.0              \n",
      " Quantity    | 1                    \n",
      " InvoiceDate | null                 \n",
      " UnitPrice   | 1.25                 \n",
      " CustomerID  | 13953                \n",
      " Country     | null                 \n",
      "-RECORD 5---------------------------\n",
      " summary     | 50%                  \n",
      " InvoiceNo   | 560686.0             \n",
      " StockCode   | 22569.0              \n",
      " Description | 20713.0              \n",
      " Quantity    | 3                    \n",
      " InvoiceDate | null                 \n",
      " UnitPrice   | 2.08                 \n",
      " CustomerID  | 15152                \n",
      " Country     | null                 \n",
      "-RECORD 6---------------------------\n",
      " summary     | 75%                  \n",
      " InvoiceNo   | 571840.0             \n",
      " StockCode   | 23165.0              \n",
      " Description | 20713.0              \n",
      " Quantity    | 10                   \n",
      " InvoiceDate | null                 \n",
      " UnitPrice   | 4.13                 \n",
      " CustomerID  | 16791                \n",
      " Country     | null                 \n",
      "-RECORD 7---------------------------\n",
      " summary     | max                  \n",
      " InvoiceNo   | C581569              \n",
      " StockCode   | m                    \n",
      " Description | wrongly sold sets    \n",
      " Quantity    | 80995                \n",
      " InvoiceDate | 9/9/2011 9:52        \n",
      " UnitPrice   | 38970.0              \n",
      " CustomerID  | 18287                \n",
      " Country     | Unspecified          \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.summary().show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fbd26",
   "metadata": {},
   "source": [
    "An DataFrame aggregation example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fa47b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa18455",
   "metadata": {},
   "source": [
    "## Aggregation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b76dd3",
   "metadata": {},
   "source": [
    "### Count\n",
    "Count on a single column will not count null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebbfd6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+--------+\n",
      "|count(CustomerID)|count(StockCode)|count(1)|\n",
      "+-----------------+----------------+--------+\n",
      "|           406829|          541909|  541909|\n",
      "+-----------------+----------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 139:>                                                        (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.select(count('CustomerID'), count('StockCode'), count('*')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb4d02",
   "metadata": {},
   "source": [
    "### countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01125664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 142:==============================================>      (175 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n",
      "CPU times: user 10 ms, sys: 2.89 ms, total: 12.9 ms\n",
      "Wall time: 2.35 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 142:===================================================> (196 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "%time df.select(countDistinct('StockCode')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc2b97",
   "metadata": {},
   "source": [
    "### approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb519095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n",
      "CPU times: user 4.9 ms, sys: 141 Âµs, total: 5.04 ms\n",
      "Wall time: 701 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "# the second parameter is the maximum estimation error allowed\n",
    "%time df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fc479",
   "metadata": {},
   "source": [
    "### first and last\n",
    "\n",
    "The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af66833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------+\n",
      "|first(Quantity, false)|last(Quantity, false)|\n",
      "+----------------------+---------------------+\n",
      "|                     6|                    3|\n",
      "+----------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"Quantity\"), last(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "250fc0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 149:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(df.head()['Quantity'])\n",
    "print(df.tail(1)[0]['Quantity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce11b3",
   "metadata": {},
   "source": [
    "### min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df8e0eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13ef10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e08386d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad323efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sumDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dd1809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 155:===============================================>     (179 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db6552",
   "metadata": {},
   "source": [
    "### Average\n",
    "Different ways of calculating the average: sum/count, avg and mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba4f861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    "  .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e314fa",
   "metadata": {},
   "source": [
    "### Variance and Standard Deviation\n",
    "For entire population and for a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2675cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609354| 47559.39140929905|  218.08095663447864|   218.08115785023486|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc2e22",
   "metadata": {},
   "source": [
    "### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb27e0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 161:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085637639E-4|             1052.7280543913773|            1052.7260778752732|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 161:============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "    covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4023aff",
   "metadata": {},
   "source": [
    "## Aggregating to Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f7f77",
   "metadata": {},
   "source": [
    "### Collect as list and as set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d89b810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 163:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------\n",
      " collect_set(Country)  | [Portugal, Italy,... \n",
      " collect_list(Country) | [United Kingdom, ... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd10ab",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c725a05",
   "metadata": {},
   "source": [
    "### Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d262a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 165:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "|   544578|     12365|    1|\n",
      "|   545165|     16339|   20|\n",
      "|   545289|     14732|   30|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 165:============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "221c1c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.group.GroupedData"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.groupBy(\"InvoiceNo\", \"CustomerId\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2efa32d",
   "metadata": {},
   "source": [
    "### Grouping with expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "177da943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------+\n",
      "|InvoiceNo|quantity_count|quantity_sum|\n",
      "+---------+--------------+------------+\n",
      "|   536596|             6|           9|\n",
      "|   536938|            14|         464|\n",
      "|   537252|             1|          31|\n",
      "|   537691|            20|         163|\n",
      "|   538041|             1|          30|\n",
      "|   538184|            26|         314|\n",
      "|   538517|            53|         161|\n",
      "|   538879|            19|         402|\n",
      "|   539275|             6|         156|\n",
      "|   539630|            12|         244|\n",
      "|   540499|            24|          90|\n",
      "|   540540|            22|          47|\n",
      "|  C540850|             1|          -1|\n",
      "|   540976|            48|         505|\n",
      "|   541432|             4|          49|\n",
      "|   541518|           101|        2334|\n",
      "|   541783|            35|         396|\n",
      "|   542026|             9|          69|\n",
      "|   542375|             6|          48|\n",
      "|  C542604|             8|         -64|\n",
      "+---------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quantity_count\"),\n",
    "    expr(\"sum(Quantity) as quantity_sum\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
