{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a62ebd1",
   "metadata": {},
   "source": [
    "# Create RDD\n",
    "One of the easiest ways to get RDDs is from an existing DataFrame or Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a0d517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.range(500).rdd\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57ad6b",
   "metadata": {},
   "source": [
    "However, by default, records are of type Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f24a968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records: [Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n",
      "record type: <class 'pyspark.sql.types.Row'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "records = rdd1.take(5)\n",
    "print(f'records: {records}')\n",
    "print(f'record type: {type(records[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe2e4a",
   "metadata": {},
   "source": [
    "So it probably needs to be mapped to the correct data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551732d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(500).rdd.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba697a",
   "metadata": {},
   "source": [
    "# Create an RDD from a local collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375d20b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[14] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = '''Please could you stop the noise?\n",
    "I'm trying to get some rest\n",
    "From all the unborn chicken\n",
    "Voices in my head\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "When I am king\n",
    "You will be first against the wall\n",
    "With your opinion\n",
    "Which is of no consequence at all'''.replace('\\n', ' ').split(' ')\n",
    "words = spark.sparkContext.parallelize(myCollection, 2) # the seconds parameter specifies the number of partitions\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29636e5c",
   "metadata": {},
   "source": [
    "# Read file using SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79850d53",
   "metadata": {},
   "source": [
    "Reading individual lines from text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf7e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78011"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = spark.sparkContext.textFile(\"/notebooks/covid-tweets\")\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bb860",
   "metadata": {},
   "source": [
    "Reading full files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54244e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_files = spark.sparkContext.wholeTextFiles(\"/notebooks/covid-tweets\")\n",
    "tweet_files.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe34ad",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52277861",
   "metadata": {},
   "source": [
    "### distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9b15ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', 'stop', 'noise?', 'trying', 'rest', 'in', 'head', \"What's\", 'may', 'but', 'an', 'android)', 'When', 'am', 'king', 'against', 'opinion', 'Which', 'is', 'of', 'no', 'at', 'could', 'you', 'the', \"I'm\", 'to', 'get', 'some', 'From', 'all', 'unborn', 'chicken', 'Voices', 'my', 'that?', '(I', 'be', 'paranoid,', 'not', 'I', 'You', 'will', 'first', 'wall', 'With', 'your', 'consequence']\n",
      "count: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "d_words = words.distinct()\n",
    "print(d_words.collect())\n",
    "print(f'count: {d_words.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910885b3",
   "metadata": {},
   "source": [
    "### filter\n",
    "select a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81522ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all', 'an', 'android)', 'an', 'android)', 'am', 'against', 'at', 'all']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFilterFunc(word):\n",
    "    # filter out each word not starting with 's'\n",
    "    return word.lower().startswith('a')\n",
    "\n",
    "f_words = words.filter(myFilterFunc)\n",
    "f_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0709dad",
   "metadata": {},
   "source": [
    "### map\n",
    "one-to-one transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8233924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'please'), ('c', 'could'), ('y', 'you'), ('s', 'stop'), ('t', 'the')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myMapFunc(word):\n",
    "    # map word to a tuple of a lower case first letter and word\n",
    "    word = word.lower()\n",
    "    return word[0], word\n",
    "m_words = words.map(myMapFunc)\n",
    "m_words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefad98b",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "one-to-many transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b19664a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial number of records: 62\n",
      "some letters:['p', 'l', 'e', 'a', 's', 'e', 'c', 'o', 'u', 'l', 'd', 'y', 'o', 'u', 's', 't', 'o', 'p', 't', 'h']\n",
      "final number of records: 253\n"
     ]
    }
   ],
   "source": [
    "print(f'initial number of records: {words.count()}')\n",
    "letters = words.flatMap(lambda word: list(word.lower())) # map a word to a series of lower case letters\n",
    "print(f'some letters:{letters.take(20)}')\n",
    "print(f'final number of records: {letters.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4537f",
   "metadata": {},
   "source": [
    "### sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683d7925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consequence',\n",
       " 'paranoid,',\n",
       " 'paranoid,',\n",
       " 'android)',\n",
       " 'android)',\n",
       " 'chicken',\n",
       " 'against',\n",
       " 'opinion',\n",
       " 'Please',\n",
       " 'noise?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by word length in descending length order\n",
    "s_words = words.sortBy(lambda word: len(word) * -1)\n",
    "s_words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70593d43",
   "metadata": {},
   "source": [
    "### randomSplit\n",
    "Create a given number of RDDs containing a number of elements based on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7372df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 10\n",
      "1: 52\n"
     ]
    }
   ],
   "source": [
    "split_rdds = words.randomSplit([0.2, 0.8]) # two RDDs with different sizes based on the given weights\n",
    "for i, split_rdd in enumerate(split_rdds):\n",
    "    print(f'{i}: {split_rdd.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a762b1",
   "metadata": {},
   "source": [
    "## Actions\n",
    "Actions either collect data to the driver or write to an external data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a17fb0",
   "metadata": {},
   "source": [
    "### reduce\n",
    "“reduce” an RDD of any kind of value to one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c04f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e1657",
   "metadata": {},
   "source": [
    "Example: get the longest word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8499f40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'consequence'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordLengthReducer(leftWord, rightWord):\n",
    "  if len(leftWord) > len(rightWord):\n",
    "    return leftWord\n",
    "  return rightWord\n",
    "\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e75beb4",
   "metadata": {},
   "source": [
    "### count, collect and take\n",
    "Too many examples by now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947a9cd",
   "metadata": {},
   "source": [
    "### countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fde424e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Please': 1,\n",
       "             'could': 1,\n",
       "             'you': 1,\n",
       "             'stop': 1,\n",
       "             'the': 3,\n",
       "             'noise?': 1,\n",
       "             \"I'm\": 1,\n",
       "             'trying': 1,\n",
       "             'to': 1,\n",
       "             'get': 1,\n",
       "             'some': 1,\n",
       "             'rest': 1,\n",
       "             'From': 1,\n",
       "             'all': 2,\n",
       "             'unborn': 1,\n",
       "             'chicken': 1,\n",
       "             'Voices': 1,\n",
       "             'in': 1,\n",
       "             'my': 1,\n",
       "             'head': 1,\n",
       "             \"What's\": 2,\n",
       "             'that?': 2,\n",
       "             '(I': 2,\n",
       "             'may': 2,\n",
       "             'be': 3,\n",
       "             'paranoid,': 2,\n",
       "             'but': 2,\n",
       "             'not': 2,\n",
       "             'an': 2,\n",
       "             'android)': 2,\n",
       "             'When': 1,\n",
       "             'I': 1,\n",
       "             'am': 1,\n",
       "             'king': 1,\n",
       "             'You': 1,\n",
       "             'will': 1,\n",
       "             'first': 1,\n",
       "             'against': 1,\n",
       "             'wall': 1,\n",
       "             'With': 1,\n",
       "             'your': 1,\n",
       "             'opinion': 1,\n",
       "             'Which': 1,\n",
       "             'is': 1,\n",
       "             'of': 1,\n",
       "             'no': 1,\n",
       "             'consequence': 1,\n",
       "             'at': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf640c",
   "metadata": {},
   "source": [
    "### first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fb30ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66fb3a",
   "metadata": {},
   "source": [
    "### takeOrdered, top and takeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9dffee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take(5):  ['Please', 'could', 'you', 'stop', 'the']\n",
      "takeOrdered:  ['(I', '(I', 'From', 'I', \"I'm\"]\n",
      "top(5):  ['your', 'you', 'will', 'wall', 'unborn']\n",
      "takeSample:  ['trying', \"What's\", 'first', 'but', 'rest', 'could']\n"
     ]
    }
   ],
   "source": [
    "print('take(5): ', words.take(5))\n",
    "print('takeOrdered: ', words.takeOrdered(5))\n",
    "print('top(5): ', words.top(5))\n",
    "withReplacement = True\n",
    "numberToTake = 6\n",
    "randomSeed = 100\n",
    "print('takeSample: ', words.takeSample(withReplacement, numberToTake, randomSeed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfbbf6c",
   "metadata": {},
   "source": [
    "## Saving Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76eeb94",
   "metadata": {},
   "source": [
    "### saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9bee6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.saveAsTextFile(\"file:/notebooks/paranoid_android3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febdec3",
   "metadata": {},
   "source": [
    "### saveAsObjectFile\n",
    "Saves the file as a Hadoop sequence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7807c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sequence file consist of key-value pairs, so words first is mapped to that format\n",
    "words.map(lambda x: (None, x)).saveAsSequenceFile(\"/paranoid_android_seqfile2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd9819",
   "metadata": {},
   "source": [
    "## Caching and Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46a9d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_words = words.map(lambda w: w.lower()).cache()\n",
    "lower_words.count()\n",
    "# lower_words is cached and filter will work on the cached results\n",
    "lower_words.filter(lambda w: w.startswith('a')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85722c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointing in an HDFS path\n",
    "spark.sparkContext.setCheckpointDir(\"hdfs://namenode:9000/checkpoint\")\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3f1e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/25 02:08:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:09:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:09:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:09:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:09:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:10:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:10:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:10:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:10:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:11:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:11:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:11:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:11:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:12:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:12:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:12:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:12:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:13:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:13:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:13:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:13:59 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:14:14 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:14:29 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/10/25 02:14:44 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 0:>                                                          (0 + 0) / 2]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_495/4137438104.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \"\"\"\n\u001b[0;32m-> 1141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \"\"\"\n\u001b[0;32m-> 1132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(100000000)).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
