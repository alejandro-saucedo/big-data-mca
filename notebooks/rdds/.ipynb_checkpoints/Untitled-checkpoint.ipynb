{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8745c1",
   "metadata": {},
   "source": [
    "# Create RDD\n",
    "One of the easiest ways to get RDDs is from an existing DataFrame or Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f34d1208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[41] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.range(500).rdd\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ed5c9",
   "metadata": {},
   "source": [
    "However, by default, records are of type Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c258006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records: [Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n",
      "record type: <class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "records = rdd1.take(5)\n",
    "print(f'records: {records}')\n",
    "print(f'record type: {type(records[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f68de",
   "metadata": {},
   "source": [
    "So it probably needs to be mapped to the correct data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "177bffc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[48] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(500).rdd.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d10068",
   "metadata": {},
   "source": [
    "# Create an RDD from a local collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0556345b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[51] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = '''Please could you stop the noise?\n",
    "I'm trying to get some rest\n",
    "From all the unborn chicken\n",
    "Voices in my head\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "When I am king\n",
    "You will be first against the wall\n",
    "With your opinion\n",
    "Which is of no consequence at all'''.split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2) # the seconds parameter specifies the number of partitions\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6634dd4",
   "metadata": {},
   "source": [
    "# Read file using SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16a0749b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_get_sep',\n",
       " '_joinrealpath',\n",
       " '_varprog',\n",
       " '_varprogb',\n",
       " 'abspath',\n",
       " 'altsep',\n",
       " 'basename',\n",
       " 'commonpath',\n",
       " 'commonprefix',\n",
       " 'curdir',\n",
       " 'defpath',\n",
       " 'devnull',\n",
       " 'dirname',\n",
       " 'exists',\n",
       " 'expanduser',\n",
       " 'expandvars',\n",
       " 'extsep',\n",
       " 'genericpath',\n",
       " 'getatime',\n",
       " 'getctime',\n",
       " 'getmtime',\n",
       " 'getsize',\n",
       " 'isabs',\n",
       " 'isdir',\n",
       " 'isfile',\n",
       " 'islink',\n",
       " 'ismount',\n",
       " 'join',\n",
       " 'lexists',\n",
       " 'normcase',\n",
       " 'normpath',\n",
       " 'os',\n",
       " 'pardir',\n",
       " 'pathsep',\n",
       " 'realpath',\n",
       " 'relpath',\n",
       " 'samefile',\n",
       " 'sameopenfile',\n",
       " 'samestat',\n",
       " 'sep',\n",
       " 'split',\n",
       " 'splitdrive',\n",
       " 'splitext',\n",
       " 'stat',\n",
       " 'supports_unicode_filenames',\n",
       " 'sys']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dir(os.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13b2f596",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__version__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_311/1250273807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '__version__' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
