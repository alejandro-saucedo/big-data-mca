{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c980d694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = '''Please could you stop the noise?\n",
    "I'm trying to get some rest\n",
    "From all the unborn chicken\n",
    "Voices in my head\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "When I am king\n",
    "You will be first against the wall\n",
    "With your opinion\n",
    "Which is of no consequence at all'''.replace('\\n', ' ').split(' ')\n",
    "words = spark.sparkContext.parallelize(myCollection, 3) # the second parameter specifies the number of partitions\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9e9b9",
   "metadata": {},
   "source": [
    "## Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd73b2d",
   "metadata": {},
   "source": [
    "### mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afee885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"partition: 0 => Please could you stop the noise? I'm trying to get some rest From all the unborn chicken Voices in my head What's that? (I may be paranoid, but not an android)\",\n",
       " \"partition: 1 => What's that? (I may be paranoid, but not an android) When I am king You will be first against the wall With your opinion Which is of no consequence at all\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_by_partition(partition_index, records):\n",
    "    words_in_part = ' '.join(records)\n",
    "    return [f'partition: {partition_index} => {words_in_part}']\n",
    "words.mapPartitionsWithIndex(text_by_partition).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51d1ee",
   "metadata": {},
   "source": [
    "### foreachPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1029891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def write_text_by_parition(records):\n",
    "    import random\n",
    "    file_name = f'/notebooks/foreachpartition_out/{str(random.random())[2:]}.txt'\n",
    "    with open(file_name, 'w') as file:\n",
    "        for word in records:\n",
    "            file.write(f'{word}\\n')\n",
    "        \n",
    "\n",
    "words.foreachPartition(write_text_by_parition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710c8e4",
   "metadata": {},
   "source": [
    "## Key-Value RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148246e3",
   "metadata": {},
   "source": [
    "### Creating a KV RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa37037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('please', 1), ('could', 1), ('you', 1), ('stop', 1), ('the', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: (word.lower(), 1)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb13d0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'Please'), ('c', 'could'), ('y', 'you'), ('s', 'stop'), ('t', 'the')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword = words.keyBy(lambda word: word.lower()[0])\n",
    "keyword.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2735e",
   "metadata": {},
   "source": [
    "### Mapping over values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f549d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'PLEASE'), ('c', 'COULD'), ('y', 'YOU'), ('s', 'STOP'), ('t', 'THE')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.mapValues(lambda word: word.upper()).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89e95ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'T'),\n",
       " ('a', 'L'),\n",
       " ('a', 'N'),\n",
       " ('b', 'B'),\n",
       " ('p', 'O'),\n",
       " ('a', 'D'),\n",
       " ('a', 'I'),\n",
       " ('n', 'I'),\n",
       " ('i', 'I'),\n",
       " ('p', 'E')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.flatMapValues(lambda word: word.upper()).takeSample(True, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415542d5",
   "metadata": {},
   "source": [
    "### Extracting keys and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0692995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'c', 'y', 's', 't']\n",
      "['Please', 'could', 'you', 'stop', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(keyword.keys().take(5))\n",
    "print(keyword.values().take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64faac56",
   "metadata": {},
   "source": [
    "### lookup\n",
    "Note that there is no enforcement mechanism with respect to there being only one key for each input, so if we lookup “s”, we are going to get both values associated with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561a78e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stop', 'some']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.lookup(\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313c6fe",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b491cb",
   "metadata": {},
   "source": [
    "### countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30939b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'p': 5,\n",
       "             'l': 10,\n",
       "             'e': 19,\n",
       "             'a': 23,\n",
       "             's': 12,\n",
       "             'c': 7,\n",
       "             'o': 22,\n",
       "             'u': 8,\n",
       "             'd': 8,\n",
       "             'y': 7,\n",
       "             't': 22,\n",
       "             'h': 13,\n",
       "             'n': 22,\n",
       "             'i': 22,\n",
       "             '?': 3,\n",
       "             \"'\": 3,\n",
       "             'm': 7,\n",
       "             'r': 10,\n",
       "             'g': 4,\n",
       "             'f': 3,\n",
       "             'b': 6,\n",
       "             'k': 2,\n",
       "             'v': 1,\n",
       "             'w': 7,\n",
       "             '(': 2,\n",
       "             ',': 2,\n",
       "             ')': 2,\n",
       "             'q': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = words.flatMap(lambda word: word.lower())\n",
    "kv_chars = chars.map(lambda letter: (letter, 1))\n",
    "kv_chars.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5882758",
   "metadata": {},
   "outputs": [],
   "source": [
    "### groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ab00e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('h', <pyspark.resultiterable.ResultIterable at 0x7ff1b4bcc390>),\n",
       " ('s', <pyspark.resultiterable.ResultIterable at 0x7ff1b4bcc490>),\n",
       " ('?', <pyspark.resultiterable.ResultIterable at 0x7ff1b4bcc090>),\n",
       " ('i', <pyspark.resultiterable.ResultIterable at 0x7ff1b4bcc350>),\n",
       " ('y', <pyspark.resultiterable.ResultIterable at 0x7ff1b4bcc050>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_chars.groupByKey().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cdb162",
   "metadata": {},
   "source": [
    "Now sum up all the records in a group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "860ec08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 5, pyspark.resultiterable.ResultIterable),\n",
       " ('l', 10, pyspark.resultiterable.ResultIterable),\n",
       " ('s', 12, pyspark.resultiterable.ResultIterable),\n",
       " ('c', 7, pyspark.resultiterable.ResultIterable),\n",
       " ('d', 8, pyspark.resultiterable.ResultIterable)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_key_values(row):\n",
    "    key = row[0]\n",
    "    values = row[1]\n",
    "    total = sum(v for v in values)\n",
    "    # note that values are collected in memory, which may result in an memory error when there are too many values for a key\n",
    "    return key, total, type(values)\n",
    "kv_chars.groupByKey().map(sum_key_values).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f2cc1",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "A safe alternative to summing using groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd4f8247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 5),\n",
       " ('l', 10),\n",
       " ('s', 12),\n",
       " ('c', 7),\n",
       " ('d', 8),\n",
       " ('y', 7),\n",
       " ('h', 13),\n",
       " ('i', 22),\n",
       " ('?', 3),\n",
       " ('r', 10)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groups of values are not loaded into memory\n",
    "kv_chars.reduceByKey(lambda left_value, right_value: left_value + right_value ).take(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2d5e8",
   "metadata": {},
   "source": [
    "### aggregate\n",
    "This function requires a null and start value. \n",
    "It operates in two levels. The first aggregates within partitions, the second aggregates across partitions. \n",
    "The start value will be used at both aggregation levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "573a4057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greatest values per partition: [5, 10, 15, 20, 25, 30]\n",
      "summation of greatest values per partition: 105\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize(range(1,31), 6)\n",
    "\n",
    "# sum the greatest values per partition\n",
    "\n",
    "def max_func(left, right):\n",
    "  return max(left, right)\n",
    "\n",
    "def add_func(left, right):\n",
    "  return left + right\n",
    "\n",
    "values_per_part = nums.mapPartitions(lambda records: [max(records)]).collect()\n",
    "print(f'greatest values per partition: {values_per_part}')\n",
    "\n",
    "aggr_result = nums.aggregate(0, max_func, add_func)\n",
    "\n",
    "print(f'summation of greatest values per partition: {aggr_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e630a9a9",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08251215",
   "metadata": {},
   "source": [
    "### cogroup\n",
    "groups together two key–value RDDs. \n",
    "This joins the given values by key. This is effectively just a group-based join on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c68f574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cogroup result:\n",
      "('p', (<pyspark.resultiterable.ResultIterable object at 0x7ff1b5015610>, <pyspark.resultiterable.ResultIterable object at 0x7ff1b4c28210>))\n",
      "('l', (<pyspark.resultiterable.ResultIterable object at 0x7ff1b4c28110>, <pyspark.resultiterable.ResultIterable object at 0x7ff1b50156d0>))\n",
      "('s', (<pyspark.resultiterable.ResultIterable object at 0x7ff1b5015750>, <pyspark.resultiterable.ResultIterable object at 0x7ff1b5015a90>))\n",
      "('c', (<pyspark.resultiterable.ResultIterable object at 0x7ff1b5015bd0>, <pyspark.resultiterable.ResultIterable object at 0x7ff1b5015c50>))\n",
      "('y', (<pyspark.resultiterable.ResultIterable object at 0x7ff1b4ff0890>, <pyspark.resultiterable.ResultIterable object at 0x7ff1b4ff0450>))\n",
      "\n",
      "cogroup result interpreted:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"p: ['0.42341080152974664', '0.024821088908647093']\",\n",
       " \"l: ['0.4372471723329814', '0.8387213090509738']\",\n",
       " \"s: ['0.6191467183510945', '0.3837784985338819']\",\n",
       " \"c: ['0.457770202543789', '0.34687334516793533']\",\n",
       " \"y: ['0.17051305281148943', '0.857335437314308']\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "distinct_chars = words.flatMap(lambda word: word.lower()).distinct()\n",
    "char_rdd_1 = distinct_chars.map(lambda c: (c, random.random()))\n",
    "char_rdd_2 = distinct_chars.map(lambda c: (c, random.random()))\n",
    "group = char_rdd_1.cogroup(char_rdd_2)\n",
    "print('cogroup result:')\n",
    "print(\"\\n\".join([str(r) for r in group.take(5)]))\n",
    "\n",
    "def pretty_output(row):\n",
    "    key = row[0]\n",
    "    groups = row[1] # groups consist of nested iterables\n",
    "    values = [str(v) for group in groups for v in group]\n",
    "    return f'{key}: {values}'\n",
    "print('\\ncogroup result interpreted:')\n",
    "group.map(pretty_output).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac6182",
   "metadata": {},
   "source": [
    "### innerJoin\n",
    "joins allow setting the number of output partitions.\n",
    "All join types (inner, fullOuter, leftOuter and rightOuter) follow the same pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67361696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "kv_chars = words.distinct().map(lambda c: (c, random.random()))\n",
    "output_partitions = 10\n",
    "print(kv_chars.count())\n",
    "print(kv_chars.join(kv_chars).count())\n",
    "print(kv_chars.join(kv_chars, output_partitions).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204bd283",
   "metadata": {},
   "source": [
    "### zip\n",
    "Groups two rdds with the same number of partitions and cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c3ff037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 10),\n",
       " (1, 11),\n",
       " (2, 12),\n",
       " (3, 13),\n",
       " (4, 14),\n",
       " (5, 15),\n",
       " (6, 16),\n",
       " (7, 17),\n",
       " (8, 18),\n",
       " (9, 19)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range1 = sc.parallelize(range(10), 2)\n",
    "range2 = sc.parallelize(range(10, 20), 2)\n",
    "range1.zip(range2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752e830",
   "metadata": {},
   "source": [
    "## Controlling Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0bfab",
   "metadata": {},
   "source": [
    "### coalesce\n",
    "Collapses partitions, trying to reduce the amount of data moved across nodes. However, partitions may end up skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68054edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number partitions with size:  [20, 20, 22]\n",
      "partitions woth size after coalesce:  [20, 42]\n"
     ]
    }
   ],
   "source": [
    "partsSizes = words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('original number partitions with size: ', partsSizes)\n",
    "# collapse from 3 to 2 partitions\n",
    "coal_words = words.coalesce(2)\n",
    "coal_partsSizes = coal_words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('partitions with size after coalesce: ', coal_partsSizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41191843",
   "metadata": {},
   "source": [
    "### repartition\n",
    "Allows a repartition in th data up or down but performs a shuffle across nodes in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb150210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number partitions with size:  [20, 20, 22]\n",
      "partitions with size after collapsing with repartition:  [32, 30]\n"
     ]
    }
   ],
   "source": [
    "partsSizes = words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('original number partitions with size: ', partsSizes)\n",
    "# collapse from 3 to 2 partitions\n",
    "collapsed_words = words.repartition(2)\n",
    "collapsed_partsSizes = collapsed_words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('partitions with size after collapsing with repartition: ', collapsed_partsSizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dedbfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitions with size after expanding with repartition:  [0, 10, 20, 20, 12]\n"
     ]
    }
   ],
   "source": [
    "exp_words = words.repartition(5)\n",
    "exp_partsSizes = exp_words.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('partitions with size after expanding with repartition: ', exp_partsSizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82380e40",
   "metadata": {},
   "source": [
    "### Custom Partitioning\n",
    "\n",
    "Custom partitioning allows fine grained control over how data is distributed. It may be usefull to handle data skew or data movement.\n",
    "\n",
    "The following example loads a list of customers. There are two customer that need to be put in their own partitions are they may fill up the memory due to their number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e72835b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:>                                                       (0 + 11) / 11]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: \n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retail RDD partitions with size: [50537, 50905, 50816, 50752, 51077, 50843, 50652, 49917, 49654, 86756]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/notebooks/online-retail-dataset.csv\")\n",
    "print('Schema: ')\n",
    "df.printSchema()\n",
    "# lets collapse to simulate data skew\n",
    "retail = df.coalesce(10).rdd\n",
    "partsSizes = retail.mapPartitions(lambda records: [sum(1 for _ in records)]).collect()\n",
    "print('Retail RDD partitions with size:', partsSizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d4838",
   "metadata": {},
   "source": [
    "A partitioner will be used to put two specific customers in the same partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9cae7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retail RDD partitions with size: ['0: 563', '1: 180613', '2: 180053', '3: 180680']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# partitioner\n",
    "def partitioning_func(key):\n",
    "  import random\n",
    "  if key == 17850 or key == 12583:\n",
    "    # records which belong to these customer go to partition 0\n",
    "    return 0\n",
    "  else:\n",
    "    # any other customer's records go to whatever other partition number from 1 to 3\n",
    "    return random.randint(1,3)\n",
    "\n",
    "# convert retailt RDD to a key-value RDD using the customer number as key\n",
    "kv_retail = retail.keyBy(lambda row: row[6])\n",
    "\n",
    "repart_kv_retail = kv_retail.partitionBy(4, partitioning_func)\n",
    "\n",
    "partsSizes = repart_kv_retail.mapPartitionsWithIndex(lambda idx, records: [f'{idx}: {sum(1 for _ in records)}']).collect()\n",
    "print('Retail RDD partitions with size:', partsSizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422f72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
