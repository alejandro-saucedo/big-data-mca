{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498db653",
   "metadata": {},
   "source": [
    "# Create RDD\n",
    "One of the easiest ways to get RDDs is from an existing DataFrame or Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19db5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.range(500).rdd\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dfa5f",
   "metadata": {},
   "source": [
    "However, by default, records are of type Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861430d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records: [Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n",
      "record type: <class 'pyspark.sql.types.Row'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "records = rdd1.take(5)\n",
    "print(f'records: {records}')\n",
    "print(f'record type: {type(records[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef769c6",
   "metadata": {},
   "source": [
    "So it probably needs to be mapped to the correct data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357c9f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(500).rdd.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484bdc0",
   "metadata": {},
   "source": [
    "# Create an RDD from a local collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd27a3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[14] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = '''Please could you stop the noise?\n",
    "I'm trying to get some rest\n",
    "From all the unborn chicken\n",
    "Voices in my head\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "What's that?\n",
    "(I may be paranoid, but not an android)\n",
    "When I am king\n",
    "You will be first against the wall\n",
    "With your opinion\n",
    "Which is of no consequence at all'''.replace('\\n', ' ').split(' ')\n",
    "words = spark.sparkContext.parallelize(myCollection, 2) # the seconds parameter specifies the number of partitions\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53760876",
   "metadata": {},
   "source": [
    "# Read file using SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaceb1b1",
   "metadata": {},
   "source": [
    "Reading individual lines from text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d7ab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78011"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = spark.sparkContext.textFile(\"/notebooks/covid-tweets\")\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e2c86",
   "metadata": {},
   "source": [
    "Reading full files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd86da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_files = spark.sparkContext.wholeTextFiles(\"/notebooks/covid-tweets\")\n",
    "tweet_files.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1fd9e",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91cfc7c",
   "metadata": {},
   "source": [
    "### distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b53a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', 'stop', 'noise?', 'trying', 'rest', 'in', 'head', \"What's\", 'may', 'but', 'an', 'android)', 'When', 'am', 'king', 'against', 'opinion', 'Which', 'is', 'of', 'no', 'at', 'could', 'you', 'the', \"I'm\", 'to', 'get', 'some', 'From', 'all', 'unborn', 'chicken', 'Voices', 'my', 'that?', '(I', 'be', 'paranoid,', 'not', 'I', 'You', 'will', 'first', 'wall', 'With', 'your', 'consequence']\n",
      "count: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "d_words = words.distinct()\n",
    "print(d_words.collect())\n",
    "print(f'count: {d_words.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b87b4f",
   "metadata": {},
   "source": [
    "### filter\n",
    "select a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f2f9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all', 'an', 'android)', 'an', 'android)', 'am', 'against', 'at', 'all']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFilterFunc(word):\n",
    "    # filter out each word not starting with 's'\n",
    "    return word.lower().startswith('a')\n",
    "\n",
    "f_words = words.filter(myFilterFunc)\n",
    "f_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095089f",
   "metadata": {},
   "source": [
    "### map\n",
    "one-to-one transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca1bb83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'please'), ('c', 'could'), ('y', 'you'), ('s', 'stop'), ('t', 'the')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myMapFunc(word):\n",
    "    # map word to a tuple of a lower case first letter and word\n",
    "    word = word.lower()\n",
    "    return word[0], word\n",
    "m_words = words.map(myMapFunc)\n",
    "m_words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae35e2",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "one-to-many transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825ee4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial number of records: 62\n",
      "some letters:['p', 'l', 'e', 'a', 's', 'e', 'c', 'o', 'u', 'l', 'd', 'y', 'o', 'u', 's', 't', 'o', 'p', 't', 'h']\n",
      "final number of records: 253\n"
     ]
    }
   ],
   "source": [
    "print(f'initial number of records: {words.count()}')\n",
    "letters = words.flatMap(lambda word: list(word.lower())) # map a word to a series of lower case letters\n",
    "print(f'some letters:{letters.take(20)}')\n",
    "print(f'final number of records: {letters.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ad27f",
   "metadata": {},
   "source": [
    "### sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ffb8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consequence',\n",
       " 'paranoid,',\n",
       " 'paranoid,',\n",
       " 'android)',\n",
       " 'android)',\n",
       " 'chicken',\n",
       " 'against',\n",
       " 'opinion',\n",
       " 'Please',\n",
       " 'noise?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by word length in descending length order\n",
    "s_words = words.sortBy(lambda word: len(word) * -1)\n",
    "s_words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c0924",
   "metadata": {},
   "source": [
    "### randomSplit\n",
    "Create a given number of RDDs containing a number of elements based on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "323784b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 10\n",
      "1: 52\n"
     ]
    }
   ],
   "source": [
    "split_rdds = words.randomSplit([0.2, 0.8]) # two RDDs with different sizes based on the given weights\n",
    "for i, split_rdd in enumerate(split_rdds):\n",
    "    print(f'{i}: {split_rdd.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b967135",
   "metadata": {},
   "source": [
    "## Actions\n",
    "Actions either collect data to the driver or write to an external data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b3aa4",
   "metadata": {},
   "source": [
    "### reduce\n",
    "“reduce” an RDD of any kind of value to one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa4d2c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c202d5",
   "metadata": {},
   "source": [
    "Example: get the longest word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38ac8938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'consequence'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordLengthReducer(leftWord, rightWord):\n",
    "  if len(leftWord) > len(rightWord):\n",
    "    return leftWord\n",
    "  return rightWord\n",
    "\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ddb1b",
   "metadata": {},
   "source": [
    "### count, collect and take\n",
    "Too many examples by now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455e149",
   "metadata": {},
   "source": [
    "### countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2772ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Please': 1,\n",
       "             'could': 1,\n",
       "             'you': 1,\n",
       "             'stop': 1,\n",
       "             'the': 3,\n",
       "             'noise?': 1,\n",
       "             \"I'm\": 1,\n",
       "             'trying': 1,\n",
       "             'to': 1,\n",
       "             'get': 1,\n",
       "             'some': 1,\n",
       "             'rest': 1,\n",
       "             'From': 1,\n",
       "             'all': 2,\n",
       "             'unborn': 1,\n",
       "             'chicken': 1,\n",
       "             'Voices': 1,\n",
       "             'in': 1,\n",
       "             'my': 1,\n",
       "             'head': 1,\n",
       "             \"What's\": 2,\n",
       "             'that?': 2,\n",
       "             '(I': 2,\n",
       "             'may': 2,\n",
       "             'be': 3,\n",
       "             'paranoid,': 2,\n",
       "             'but': 2,\n",
       "             'not': 2,\n",
       "             'an': 2,\n",
       "             'android)': 2,\n",
       "             'When': 1,\n",
       "             'I': 1,\n",
       "             'am': 1,\n",
       "             'king': 1,\n",
       "             'You': 1,\n",
       "             'will': 1,\n",
       "             'first': 1,\n",
       "             'against': 1,\n",
       "             'wall': 1,\n",
       "             'With': 1,\n",
       "             'your': 1,\n",
       "             'opinion': 1,\n",
       "             'Which': 1,\n",
       "             'is': 1,\n",
       "             'of': 1,\n",
       "             'no': 1,\n",
       "             'consequence': 1,\n",
       "             'at': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e9700",
   "metadata": {},
   "source": [
    "### first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b5d5978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dfaf49",
   "metadata": {},
   "source": [
    "### takeOrdered, top and takeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f5ab7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take(5):  ['Please', 'could', 'you', 'stop', 'the']\n",
      "takeOrdered:  ['(I', '(I', 'From', 'I', \"I'm\"]\n",
      "top(5):  ['your', 'you', 'will', 'wall', 'unborn']\n",
      "takeSample:  ['trying', \"What's\", 'first', 'but', 'rest', 'could']\n"
     ]
    }
   ],
   "source": [
    "print('take(5): ', words.take(5))\n",
    "print('takeOrdered: ', words.takeOrdered(5))\n",
    "print('top(5): ', words.top(5))\n",
    "withReplacement = True\n",
    "numberToTake = 6\n",
    "randomSeed = 100\n",
    "print('takeSample: ', words.takeSample(withReplacement, numberToTake, randomSeed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0eb05",
   "metadata": {},
   "source": [
    "## Saving Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8ef56",
   "metadata": {},
   "source": [
    "### saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8040bc6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o350.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/notebooks/paranoid_android2 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1716/1272651644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:/notebooks/paranoid_android2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o350.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/notebooks/paranoid_android2 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "words.saveAsTextFile(\"file:/notebooks/paranoid_android2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c25f46",
   "metadata": {},
   "source": [
    "### saveAsObjectFile\n",
    "Saves the file as a Hadoop sequence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sequence file consist of key-value pairs, so words first is mapped to that format\n",
    "words.map(lambda x: (None, x)).saveAsSequenceFile(\"/paranoid_android_seqfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88208dd5",
   "metadata": {},
   "source": [
    "## Caching and Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ddb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_words = words.map(lambda w: w.lower()).cache()\n",
    "lower_words.count()\n",
    "# lower_words is cached and filter will work on the cached results\n",
    "lower_words.filter(lambda w: w.startswith('a')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ba911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointing in an HDFS path\n",
    "spark.sparkContext.setCheckpointDir(\"hdfs://namenode:9000/checkpoint\")\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f7116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
