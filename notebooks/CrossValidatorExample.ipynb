{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff99c0f4-1fd7-4101-b241-ca7167a751ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528aab63-d683-4486-bef6-9b11bebe2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "\t.builder \\\n",
    "    .master('spark://spark-master:7077') \\\n",
    "\t.appName(\"CrossValidatorExample\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b8c896-bac6-43d9-95ee-98317ae22fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "\t(0, \"a b c d e spark\", 1.0),\n",
    "\t(1, \"b d\", 0.0),\n",
    "\t(2, \"spark f g h\", 1.0),\n",
    "\t(3, \"hadoop mapreduce\", 0.0),\n",
    "\t(4, \"b spark who\", 1.0),\n",
    "\t(5, \"g d a y\", 0.0),\n",
    "\t(6, \"spark fly\", 1.0),\n",
    "\t(7, \"was mapreduce\", 0.0),\n",
    "\t(8, \"e spark program\", 1.0),\n",
    "\t(9, \"a e c l\", 0.0),\n",
    "\t(10, \"spark compile\", 1.0),\n",
    "\t(11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb9241c6-e9d8-4dfe-b7ba-8a522c8e64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51e5932-6c44-4ad5-b1ae-cd249b33b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "\t.addGrid(hashingTF.numFeatures, [10, 100, 1000, 5000]) \\\n",
    "\t.addGrid(lr.regParam, [0.1, 0.01, 0.01]) \\\n",
    "    .addGrid(lr.threshold, [0.2, 0.5, 0.7]) \\\n",
    "\t.addGrid(lr.maxIter, [10, 100, 1000, 5000]) \\\n",
    "\t.build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(), numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3635c527-494a-4c50-a5e4-fbf30a9ee193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validation and hyperparameter search took:  774.7232420444489  seconds\n"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "start_time = time.time()\n",
    "cvModel = crossval.fit(training)\n",
    "end_time = time.time()\n",
    "print(\"The cross validation and hyperparameter search took: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b76d1f7-3ec2-40e2-81c0-5c90ca786c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.3407, 0.6593]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9432, 0.0568]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.3449, 0.6551]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.9563, 0.0437]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "\t(4, \"spark i j k\"),\n",
    "\t(5, \"l m n\"),\n",
    "\t(6, \"mapreduce spark\"),\n",
    "\t(7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "\tprint(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
